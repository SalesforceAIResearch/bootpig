train_batch_size: 4
max_steps: 10000
epochs: 1
gradient_accumulation_steps: 2
ref_unet_learning_rate: 5e-6
base_unet_learning_rate: 1e-7
scale_lr: False
lr_scheduler: constant
lr_warmup_steps: 200
lr_num_cycles: 1
lr_power: 1
use_8bit_adam: False
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 1e-2
adam_epsilon: 1e-8
max_grad_norm: 1.0
